<p><big><b>2.  The theory of operation</b></big></p>

<p>Why do we need CPU cache?  It solves the problem of so-called
"processor-memory performance gap".  The issue is: during the past decades
the speed of CPUs grew faster than the speed of dynamic memory, used for
storing a large amount of data.  This difference in growth is illustrated on
<b><font color=blue>Figure 1</font></b>.  As a result, without cache, a
typical transaction (fetch, load or store) from CPU to the memory could take
the same amount of time (the same number of clock cycles) as tens, sometimes
a hundred or more arithmetic operations inside the CPU.  It means that with
a typical memory-intensive application a CPU without caches would waste most
program execution time idling, waiting to complete the memory
operations.</p>

<center>

<p><b><font color=blue>Figure 1.  Processor-memory performance gap, from
Computer Architecture: A Quantitative Approach by David A. Patterson and
John L. Hennessy </font></b> </p>

<img width=500
src="http://www.silicon-russia.com/wp-content/uploads/2016/09/KgYa8BM.png"
/>

</center>

<p>One of the best descriptions of cache used in MIPS CPUs is in book <a
href="https://www.amazon.com/Second-Morgan-Kaufmann-Computer-Architecture/dp/0120884216">See
MIPS Run, Second Edition by Dominic Sweetman</a>.  A fragment of this
description is below:</p>

<blockquote>

<p>The cache's job is to keep a copy of memory data that has been recently
read or written, so it can be returned to the CPU quickly.  For L1 caches,
the read must complete in a fixed period of time to keep the pipeline
running.</p>

<p>MIPS CPUs always have separate L1 caches for instructions and data
(I-cache and D-cache, respectively) so that an instruction can be read and a
load or store done simultaneously.</p>

<p>Conceptually, a cache is an associative memory, a chunk of storage where
data is deposited and can be found again using an arbitrary data pattern as
a key.  In a cache, the key is the full memory address.  Produce the same
key back to an associative memory and you'll get the same data back again. 
A real associative memory will accept a new item using an arbitrary key, at
least until it's full; however, since a presented key has to be compared
with every stored key simultaneously, a genuine associative memory of any
size is either hopelessly resource hungry, slow, or both.</p>

<p>So how can we make a useful cache that is fast and efficient?  <b><font
color=blue>Figure 2</font></b> shows the basic layout of the simplest kind
of cache, the direct-mapped cache used in most MIPS CPUs up to the 1992
generation.</p>

<p>The direct-mapped arrangement uses a simple chunk of high-speed memory
(the cache store) indexed by enough low address bits to span its size.  Each
line inside the cache store contains one or more words of data and a cache
tag field, which records the memory address where this data belongs.</p>

<p>On a read, the cache line is accessed, and the tag field is compared with
the higher addresses of the memory address; if the tag matches, we know
we've got the right data and have "hit" in the cache.  Where there's more
than one word in the line, the appropriate word will be selected based on
the very lowest address bits.</p>

<p>If the tag doesn't match, we've missed and the data will be read from
memory and copied into the cache.  The data that was previously held in the
cache is simply discarded and will need to be fetched from memory again if
the CPU references it.</p>

<center>

<p><b><font color=blue>Figure 2.  Direct-mapped cache.</font></b></p>

<img width=500
src="http://www.silicon-russia.com/wp-content/uploads/2016/10/direct_mapped_cache.png"
/>

</center>

<p>A direct-mapped cache like this one has the property that, for any given
memory address, there is only one line in the cache store where that data
can be kept.<sup>1</sup> That might be good or bad; it's good because such a
simple structure will be fast and will allow us to run the whole CPU faster. 
But simplicity has its bad side too: If your program makes repeated
reference to two data items that happen to share the same cache location
(presumably because the low bits of their addresses happen to be close
together), then the two data items will keep pushing each other out of the
cache and efficiency will fall drastically.  A real associative memory
wouldn't suffer from this kind of thrashing, but it is too slow and
expensive.</p>

<p>A common compromise is to use a two-way set-associative cacheâ€”which is
really just a matter of running two direct-mapped caches in parallel and
looking up memory locations in both of them, as shown in <b><font
color=blue>Figure 3</font></b>.</p>

<sup>1.</sup> <small>In a fully associative memory, data associated with any
given memory address (key) can be stored anywhere; a direct-mapped cache is
as far from being content addressable as a cache store can be.</small>

<center>

<p><b><font color=blue>Figure 3.  Two-way set-associative
cache.</font></b></p>

<img width=500
src="http://www.silicon-russia.com/wp-content/uploads/2016/10/2_way_set_associative_cache.png"
/>

</center>

<p>Now we've got two chances of getting a hit on any address.  Four-way
setassociative caches (where there are effectively four direct-mapped
subcaches) are also common in on-chip caches.</p>

<p>In a multiway cache there's more than one choice of the cache location to
be used in fixing up a cache miss, and thus more than one acceptable choice
of the cache line to be discarded.  The ideal solution is probably to keep
track of accesses to cache lines and pick the "least recently used" ("LRU")
line to be replaced, but maintaining strict LRU order means updating LRU
bits in every cache line every time the cache is read.  Moreover, keeping
strict LRU information in a more-than-four-way set-associative cache becomes
impractical.  Real caches often use compromise algorithms like "least
recently filled" to pick the line to discard.</p>

</blockquote>

<p>CPU caches exploit so-called principle of locality of memory access
patterns found in almost all programs.  Below is the explanation for two
major types of locality from Wikipedia and <b><font color=blue>Figure
4</font></b> that illustrates the patterns with more specifics.  The
understanding of this principle is relevant to this lab because it explains,
for example, why MIPSfpga cache loads four words on cache miss (the cache
line) rather than just one word.</p>

<blockquote>

<p>From <a
href="https://en.wikipedia.org/wiki/Locality_of_reference">https://en.wikipedia.org/wiki/Locality_of_reference</a>:</p>

<ul>

<li>Temporal locality: If at one point a particular memory location is
referenced, then it is likely that the same location will be referenced
again in the near future.</li>

<li>Spatial locality: If a particular storage location is referenced at a
particular time, then it is likely that nearby memory locations will be
referenced in the near future.</li>

</ul>

</blockquote>

<center>

<p><b><font color=blue>Figure 4.  Memory access patterns from <a
href="https://www.coursera.org/learn/comparch">Computer Architecture</a>
course by David Wentzlaff from Princeton University.  2011.</font></b></p>

<img width=500
src="http://www.silicon-russia.com/wp-content/uploads/2016/10/cache_locality.png"
/>

</center>

<p>Finally, <b><font color=blue>Figure 5</font></b> shows the structures
associated with each cache line in MIPS microAptiv UP processor core.  The
particular configuration of this core used in MIPSfpga has 2-way
set-associativity for both instruction and data caches; each cache way has
size of 2 KB in both I-cache and D-cache.  The parameters of caches used
during the synthesis can be found in Verilog header file
<i>m14k_config.vh</i> located in the directory <i>core_rtl</i>:</p>

<blockquote><p>File <i>core_rtl/m14k_config.vh</i></p>
<pre>                           

// *************************************************************************
// Cache module parameters
// Not used with scache or cache stub modules
// *************************************************************************
// Cache Associativity
//      1-4 way set associative

`define M14K_ICACHE_ASSOC 2
`define M14K_DCACHE_ASSOC 2

// Cache Way Size
//      Size/Way in KB
//      1,2,4,8,16 KB

`define M14K_ICACHE_WAYSIZE 2
`define M14K_DCACHE_WAYSIZE 2



// *************************************************************************
// Cache Controller parameters
// *************************************************************************
// ! If changing manually, remember to change WS Ram Width below to match !
// Maximum Cache Associativity
//      1-4 way set associative (including Spram)

`define M14K_MAX_IC_ASSOC 2
`define M14K_MAX_DC_ASSOC 2

</pre></blockquote>

<center>

<p><b><font color=blue>Figure 5.  The structures associated with each cache
line in MIPS microAptiv UP processor core processor core.</font></b></p>

<img
src="http://www.silicon-russia.com/wp-content/uploads/2016/10/mips_microaptiv_up_cache.png"
/>

</center>
