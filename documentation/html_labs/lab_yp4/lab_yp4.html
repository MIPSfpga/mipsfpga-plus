<html>
<head>
<title>MIPSfpga 2.0. Lab YP4 - The first glance into caches</title>

<style>

a, body, td
{
    font-family : 'Lucida Sans Unicode', 'Lucida Grande', sans-serif;
    font-size   : 12px;
}

pre
{
    font-family : 'Lucida Console', Monaco, monospace;
    font-size   : 12px;
}

</style>
</head>
<body>
<p><big><big><b>MIPSfpga 2.0. Lab YP4 - The first glance into caches</b></big></big></p>
<p><big><b>1.  Introduction</b></big></p>

<p>This lab demonstrates how the work of CPU cache can be directly observed
in the most obvious and straigtforward fashion when running MIPSfpga-based
system on FPGA board.  During this lab a student first makes the design
clock running really slow (a dozen of beats per second), and then the
student's program fills a two-dimentional memory array in one of two
different ways: either with moving row index first or with moving column
index first.  The resulting cache hit and miss patterns, different for
raw-first and column-first runs, are clearly visible on blinking LED, which
is connected to a signal that indicates cache fill line request.</p>

<p>Each cache line in MIPSfpga CPU cache holds four 32-bit words.  As a
result, when the program fills the memory array by moving column index
first, the pattern is going to be "cache miss - hit - hit - hit - miss - hit
- hit - hit - miss ...".  However when the program fills the array by moving
raw index first, the pattern is going to be a series of cache misses
followed by a long series of cache hits, since a large portion of the array
is already in the cache.</p>

<p>This lab does not attempt to measure, quantify or analyze cache behaviour
in details.  The only goal of this lab is to create "A-ha" moment for a
person who wants to visualize for himself the role a CPU cache plays in the
computer system.  The precise analysis requires using different methods, for
example performance counters, as described in other cache-related labs in
MIPSfpga 2.0 package.</p>
<p><big><b>2. The theory of operation</b></big></p>

<p><big><b>2.1. MIPSfpga 2.0 hardware module hierarchy without Serial Loader Flow support</b></big></p>

<p><b><font color=blue>Figure 2.1</font></b> shows the basic hierarchy of synthesizable modules of MIPSfpga 2.0 for Digilent Nexys 4 DDR board with Xilinx Artix-7 FPGA and without Serial Loader hardware:</p>

<p><center><b><font color=blue>Figure 2.1</font></b></center></p>

<img width=500 src="http://www.silicon-russia.com/wp-content/uploads/2016/09/KgYa8BM.png" />

<img width=500 src="http://www.silicon-russia.com/wp-content/uploads/2016/10/cache_locality.png" />

<img width=500 src="http://www.silicon-russia.com/wp-content/uploads/2016/10/direct_mapped_cache.png" />


<img src="http://www.silicon-russia.com/wp-content/uploads/2016/10/mips_microaptiv_up_cache.png" />

<img src="http://www.silicon-russia.com/wp-content/uploads/2015/11/Screenshot-2015-11-24-21.39.48.png" />

https://www.amazon.com/Second-Morgan-Kaufmann-Computer-Architecture/dp/0120884216
https://www.amazon.com/Second-Morgan-Kaufmann-Computer-Architecture/dp/0120884216
See MIPS Run, Second Edition (The Morgan Kaufmann Series in Computer Architecture and Design) 2nd Edition

Computer Architecture, Fifth Edition: A Quantitative Approach
by David A. Patterson and John L. Hennessy

David Wentzlaff 2011 from Princeton University	

<blockquote>
<p><big><b>4.1 Caches and Cache Management</b></big></p>

<p>The cache's job is to keep a copy of memory data that has been recently
read or written, so it can be returned to the CPU quickly.  For L1 caches,
the read must complete in a fixed period of time to keep the pipeline
running.</p>

<p>MIPS CPUs always have separate L1 caches for instructions and data
(I-cache and D-cache, respectively) so that an instruction can be read and a
load or store done simultaneously.</p>

<p>. . . . .</p>

<p><big><b>4.2 How Caches Work</b></big></p>

<p>Conceptually, a cache is an associative memory, a chunk of storage where data
is deposited and can be found again using an arbitrary data pattern as a key. In
a cache, the key is the full memory address. Produce the same key back to an
associative memory and you'll get the same data back again. A real associative
memory will accept a new item using an arbitrary key, at least until it's full;
however, since a presented key has to be compared with every stored key simultaneously,
a genuine associative memory of any size is either hopelessly resource
hungry, slow, or both.</p>

<p>So how can we make a useful cache that is fast and efficient? Figure 4.1 shows
the basic layout of the simplest kind of cache, the direct-mapped cache used in
most MIPS CPUs up to the 1992 generation.</p>

<p>The direct-mapped arrangement uses a simple chunk of high-speed memory
(the cache store) indexed by enough low address bits to span its size. Each
line inside the cache store contains one or more words of data and a cache tag
field, which records the memory address where this data belongs.</p>

<p>On a read, the cache line is accessed, and the tag field is compared with the
higher addresses of the memory address; if the tag matches, we know we've got
the right data and have "hit" in the cache. Where there's more than one word in
the line, the appropriate word will be selected based on the very lowest address
bits.</p>

<p>If the tag doesn't match, we've missed and the data will be read from memory
and copied into the cache. The data that was previously held in the cache
is simply discarded and will need to be fetched from memory again if the CPU
references it.</p>

<center>
<p><b><font color=blue>FIGURE 4.1 Direct-mapped cache.</font></b></p>

<img width=500 src="http://www.silicon-russia.com/wp-content/uploads/2016/10/direct_mapped_cache.png" />
</center>

<p>A direct-mapped cache like this one has the property that, for any given
memory address, there is only one line in the cache store where that data
can be kept.<sup>1</sup> That might be good or bad; it's good because such a simple
structure will be fast and will allow us to run the whole CPU faster.  But
simplicity has its bad side too: If your program makes repeated reference to
two data items that happen to share the same cache location (presumably
because the low bits of their addresses happen to be close together), then
the two data items will keep pushing each other out of the cache and
efficiency will fall drastically.  A real associative memory wouldn't suffer
from this kind of thrashing, but it is too slow and expensive.</p>

<p>A common compromise is to use a two-way set-associative cacheâ€”which is
really just a matter of running two direct-mapped caches in parallel and
looking up memory locations in both of them, as shown in Figure 4.2.</p>

<sup>1.</sup> <small>In a fully associative memory, data associated with any given memory address (key) can be
stored anywhere; a direct-mapped cache is as far from being content addressable as a cache
store can be.</small>

<center>
<p><b><font color=blue>FIGURE 4.2 Two-way set-associative cache.</font></b></p>

<img width=500 src="http://www.silicon-russia.com/wp-content/uploads/2016/10/2_way_set_associative_cache.png" />
</center>

<p>Now we've got two chances of getting a hit on any address. Four-way setassociative
caches (where there are effectively four direct-mapped subcaches)
are also common in on-chip caches.</p>

<p>In a multiway cache there's more than one choice of the cache location to
be used in fixing up a cache miss, and thus more than one acceptable choice
of the cache line to be discarded. The ideal solution is probably to keep track
of accesses to cache lines and pick the "least recently used" ("LRU") line to be
replaced, but maintaining strict LRU order means updating LRU bits in every
cache line every time the cache is read. Moreover, keeping strict LRU information
in a more-than-four-way set-associative cache becomes impractical. Real
caches often use compromise algorithms like "least recently filled" to pick the
line to discard.</p>
</blockquote>

<p><big><b>3. Lab steps</b></big></p>

<p>This section outlines the sequence of steps, necessary to complete the
lab.  Almost all generic steps in this lab are the same as in <i>MIPSfpga
2.0 Lab YP1.  Using MIPSfpga with Serial Loader Flow that does not require
BusBlaster board and OpenOCD software</i>.  Such generic steps are not
described in this section.  Only the steps different from <i>Lab YP1</i> are
explained in details.</p>

<p><big><b>3.1. Selecting the appropriate hardware configuration for the lab</b></big></p>

<p>Before running synthesis it is necessary to review and possibly modify the file
<a href="http://github.com/MIPSfpga/mipsfpga-plus/blob/master/mfp_ahb_lite_matrix_config.vh">mfp_ahb_lite_matrix_config.vh</a>
that includes a set of Verilog  <i>`define</i> statements that determine the functionality
of the synthesized MIPSfpga system. The configuration should be the
following:</p>

<blockquote><p>File <i><a href="http://github.com/MIPSfpga/mipsfpga-plus/blob/master/mfp_ahb_lite_matrix_config.vh">mfp_ahb_lite_matrix_config.vh</a></i></p>
<pre>                           

//
//  Configuration parameters
//

// `define MFP_USE_WORD_MEMORY
// `define MFP_INITIALIZE_MEMORY_FROM_TXT_FILE
   `define MFP_USE_SLOW_CLOCK_AND_CLOCK_MUX
   `define MFP_USE_UART_PROGRAM_LOADER
// `define MFP_DEMO_LIGHT_SENSOR
   `define MFP_DEMO_CACHE_MISSES
// `define MFP_DEMO_PIPE_BYPASS

</pre></blockquote>



<blockquote><p>File <i>04_disassemble.bat</i></p><pre>                                
</pre></blockquote>


</body>
</html>
